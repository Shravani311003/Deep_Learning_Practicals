{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "814ac94f-6c2a-4fad-ac99-fbaa3abe1d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: W=[1.6065621], b=[-1.4745884], Loss=5.23151969909668\n",
      "Step 500: W=[2.0189419], b=[-1.0567116], Loss=1.0421353578567505\n",
      "Step 1000: W=[2.2347412], b=[-0.8163167], Loss=0.15752272307872772\n",
      "Step 1500: W=[2.294912], b=[-0.70566016], Loss=0.07143667340278625\n",
      "Step 2000: W=[2.2860498], b=[-0.64174724], Loss=0.05942171439528465\n",
      "Step 2500: W=[2.259234], b=[-0.57829195], Loss=0.04840308427810669\n",
      "Step 3000: W=[2.2266915], b=[-0.50566447], Loss=0.03701521083712578\n",
      "Step 3500: W=[2.19048], b=[-0.42496714], Loss=0.026145631447434425\n",
      "Step 4000: W=[2.1522155], b=[-0.3396742], Loss=0.016705401241779327\n",
      "Step 4500: W=[2.1141353], b=[-0.25474033], Loss=0.009397427551448345\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Example variables for demonstration\n",
    "W = tf.Variable(tf.random.normal([1]), name='W')\n",
    "b = tf.Variable(tf.random.normal([1]), name='b')\n",
    "\n",
    "# Sample data for demonstration\n",
    "X = tf.constant([[1.0], [2.0], [3.0]], dtype=tf.float32)\n",
    "y = tf.constant([[2.0], [4.0], [6.0]], dtype=tf.float32)\n",
    "\n",
    "# Define the mean squared error loss function\n",
    "def mse_loss():\n",
    "    y_pred = W * X + b\n",
    "    return tf.reduce_mean(tf.square(y_pred - y))\n",
    "\n",
    "# Initialize the Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Training loop\n",
    "for step in range(5000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        current_loss = mse_loss()\n",
    "    \n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(current_loss, [W, b])\n",
    "    \n",
    "    # Update weights\n",
    "    optimizer.apply_gradients(zip(gradients, [W, b]))\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        # Print current values of W, b, and loss\n",
    "        print(f\"Step {step}: W={W.numpy()}, b={b.numpy()}, Loss={current_loss.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a732ef-8f7d-4d60-88f8-f042c7ad52b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
